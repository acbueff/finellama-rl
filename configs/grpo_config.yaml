# GRPO Training Configuration

# Training parameters
seed: 42
num_epochs: 5
max_steps: 20000
mini_batch_size: 8
batch_size: 128
gradient_accumulation_steps: 8
learning_rate: 1.0e-5
max_grad_norm: 1.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
weight_decay: 0.01
warmup_steps: 100
lr_scheduler_type: "cosine"
save_interval: 500
eval_interval: 200
log_interval: 10

# GRPO specific parameters
grpo_epochs: 3  # Number of GRPO epochs per batch of experiences
preference_threshold: 0.1  # Threshold for preference confidence
beta: 0.1  # Regularization coefficient for GRPO
use_preference_model: true  # Whether to use a learned preference model
preference_model_type: "bert-base"  # Type of the preference model
preference_model_lr: 5.0e-5  # Learning rate for preference model
preference_model_steps: 10  # Steps to train preference model per batch
preference_model_batch_size: 16  # Batch size for preference model training
use_rejection_sampling: true  # Whether to use rejection sampling
max_rejections: 5  # Maximum number of rejection samples

# Paired sampling parameters
num_sample_pairs: 64  # Number of sample pairs per problem
paired_batch_size: 16  # Batch size for paired samples generation
paired_temperature: 1.0  # Temperature for paired sample generation
paired_top_p: 1.0  # Top-p for paired sample generation
paired_top_k: 50  # Top-k for paired sample generation
preference_margin: 0.05  # Minimum margin for considering a preference significant

# Generation parameters for paired samples
prompt_max_length: 512  # Maximum prompt length
response_max_length: 512  # Maximum response length
temperature: 1.0  # Temperature for generation 
top_p: 1.0  # Top-p sampling parameter
do_sample: true  # Whether to use sampling

# Reward parameters (used for preference labeling)
math_correctness_weight: 1.0  # Weight for mathematical correctness reward
process_weight: 0.5  # Weight for reasoning process reward
brevity_weight: 0.1  # Weight for brevity reward
use_reference_kl: true  # Use KL divergence from reference model as penalty
kl_penalty_weight: 0.05  # Weight for reference KL penalty
max_length_penalty: 0.001  # Penalty per token beyond optimal length

# Diversity parameters
diversity_coefficient: 0.1  # Coefficient for enforcing diversity in paired samples
max_diversity_metric: "cosine"  # Metric to use for measuring diversity
min_token_difference: 10  # Minimum number of different tokens to consider samples diverse

# Output directories
output_dir: "/proj/berzelius-aiics-real/users/x_anbue/ppo_grpo/models/grpo_finetuned"
tensorboard_dir: "/proj/berzelius-aiics-real/users/x_anbue/ppo_grpo/results/grpo/tensorboard"
wandb_logging: true  # Whether to use Weights & Biases logging

# Resources
num_workers: 4  # Number of parallel workers for data loading 
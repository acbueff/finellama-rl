# PPO Training Configuration for Qwen2.5 3B on GSM8K

# Training parameters
seed: 42
num_epochs: 3  # Reduced number of epochs to avoid overfitting on GSM8K
max_steps: 5000
mini_batch_size: 4  # Reduced from 8 to 4
batch_size: 4  # Reduced from 8 to 4 to address GPU memory issues
gradient_accumulation_steps: 16  # Increased from 8 to 16
learning_rate: 2.0e-5  # Slightly higher learning rate
max_grad_norm: 1.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
weight_decay: 0.01
warmup_steps: 100
lr_scheduler_type: "cosine"
save_interval: 200
eval_interval: 100
log_interval: 10

# PPO specific parameters
ppo_epochs: 2  # Reduced from 4 to 2 to save memory
clip_range: 0.2  # PPO clipping range
value_clip_range: 0.2  # Clipping range for value function
target_kl: 0.01  # Target KL divergence threshold for early stopping
normalize_advantage: true  # Whether to normalize advantages
vf_coef: 0.5  # Value function coefficient in loss
entropy_coef: 0.01  # Entropy coefficient in loss
gamma: 0.99  # Discount factor
lam: 0.95  # GAE lambda parameter
use_adaptive_kl: true  # Use adaptive KL penalty
init_kl_coef: 0.2  # Initial KL penalty coefficient
adap_kl_target: 6.0  # Target KL for adaptive penalty

# Generation parameters for rollouts
prompt_max_length: 384  # Reduced from 512 to 384
response_max_length: 384  # Reduced from 512 to 384
temperature: 1.0  # Temperature for generation during rollouts
top_p: 1.0  # Top-p sampling parameter during rollouts
do_sample: true  # Whether to use sampling during rollouts

# Reward parameters
math_correctness_weight: 1.0  # Weight for mathematical correctness reward
process_weight: 0.5  # Weight for reasoning process reward
brevity_weight: 0.1  # Weight for brevity reward (penalizes overly verbose answers)
use_reference_kl: true  # Use KL divergence from reference model as additional penalty
kl_penalty_weight: 0.05  # Weight for reference KL penalty
max_length_penalty: 0.001  # Penalty per token beyond optimal length

# Experience collection
num_rollouts: 16  # Reduced from 32 to 16
rollout_batch_size: 4  # Reduced from 8 to 4

# Output directories
output_dir: "/proj/berzelius-aiics-real/users/x_anbue/qwen_gsm8k/models/ppo_finetuned"
tensorboard_dir: "/proj/berzelius-aiics-real/users/x_anbue/qwen_gsm8k/results/tensorboard"
wandb_logging: true  # Whether to use Weights & Biases logging

# Resources
num_workers: 4  # Number of parallel workers for data loading 